{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
    "    def __init__(self, src_path, trg_path, src_word2id, trg_word2id):\n",
    "        \"\"\"Reads source and target sequences from txt files.\"\"\"\n",
    "        self.src_seqs = open(src_path).readlines()\n",
    "        self.trg_seqs = open(trg_path).readlines()\n",
    "        self.num_total_seqs = len(self.src_seqs)\n",
    "        self.src_word2id = src_word2id\n",
    "        self.trg_word2id = trg_word2id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
    "        src_seq = self.src_seqs[index]\n",
    "        trg_seq = self.trg_seqs[index]\n",
    "        src_seq = self.preprocess(src_seq, self.src_word2id, trg=False)\n",
    "        trg_seq = self.preprocess(trg_seq, self.trg_word2id)\n",
    "        return src_seq, trg_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_total_seqs\n",
    "\n",
    "    def preprocess(self, sequence, word2id, trg=True):\n",
    "        \"\"\"Converts words to ids.\"\"\"\n",
    "        tokens = nltk.tokenize.word_tokenize(sequence.lower())\n",
    "        sequence = []\n",
    "        sequence.append(word2id['<start>'])\n",
    "        sequence.extend([word2id[token] for token in tokens if token in word2id])\n",
    "        sequence.append(word2id['<end>'])\n",
    "        sequence = torch.Tensor(sequence)\n",
    "        return sequence\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (src_seq, trg_seq).\n",
    "    We should build a custom collate_fn rather than using default collate_fn,\n",
    "    because merging sequences (including padding) is not supported in default.\n",
    "    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n",
    "    Args:\n",
    "        data: list of tuple (src_seq, trg_seq).\n",
    "            - src_seq: torch tensor of shape (?); variable length.\n",
    "            - trg_seq: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        src_seqs: torch tensor of shape (batch_size, padded_length).\n",
    "        src_lengths: list of length (batch_size); valid length for each padded source sequence.\n",
    "        trg_seqs: torch tensor of shape (batch_size, padded_length).\n",
    "        trg_lengths: list of length (batch_size); valid length for each padded target sequence.\n",
    "    \"\"\"\n",
    "    def merge(sequences):\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        padded_seqs = torch.zeros(len(sequences), max(lengths)).long()\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq[:end]\n",
    "        return padded_seqs, lengths\n",
    "\n",
    "    # sort a list by sequence length (descending order) to use pack_padded_sequence\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    # seperate source and target sequences\n",
    "    src_seqs, trg_seqs = zip(*data)\n",
    "\n",
    "    # merge sequences (from tuple of 1D tensor to 2D tensor)\n",
    "    src_seqs, src_lengths = merge(src_seqs)\n",
    "    trg_seqs, trg_lengths = merge(trg_seqs)\n",
    "    \n",
    "    batch_handler = Batch(src_seqs.transpose(0,1), trg_seqs.transpose(0,1), 1)\n",
    "\n",
    "    return batch_handler.src.transpose(0, 1), batch_handler.trg.transpose(0, 1), batch_handler.src_mask, batch_handler.trg_mask.unsqueeze(0).repeat(2, 1, 1), batch_handler.trg_y.transpose(0, 1)\n",
    "\n",
    "\n",
    "def get_loader(src_path, trg_path, src_word2id, trg_word2id, batch_size=32):\n",
    "    \"\"\"Returns data loader for custom dataset.\n",
    "    Args:\n",
    "        src_path: txt file path for source domain.\n",
    "        trg_path: txt file path for target domain.\n",
    "        src_word2id: word-to-id dictionary (source domain).\n",
    "        trg_word2id: word-to-id dictionary (target domain).\n",
    "        batch_size: mini-batch size.\n",
    "    Returns:\n",
    "        data_loader: data loader for custom dataset.\n",
    "    \"\"\"\n",
    "    # build a custom dataset\n",
    "    dataset = Dataset(src_path, trg_path, src_word2id, trg_word2id)\n",
    "\n",
    "    # data loader for custome dataset\n",
    "    # this will return (src_seqs, src_lengths, trg_seqs, trg_lengths) for each iteration\n",
    "    # please see collate_fn for details\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import argparse\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_word2id(seq_path, min_word_count):\n",
    "    \"\"\"Creates word2id dictionary.\n",
    "    \n",
    "    Args:\n",
    "        seq_path: String; text file path\n",
    "        min_word_count: Integer; minimum word count threshold\n",
    "        \n",
    "    Returns:\n",
    "        word2id: Dictionary; word-to-id dictionary\n",
    "    \"\"\"\n",
    "    sequences = open(seq_path).readlines()\n",
    "    num_seqs = len(sequences)\n",
    "    counter = Counter()\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        tokens = nltk.tokenize.word_tokenize(sequence.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the sequences.\".format(i, num_seqs))\n",
    "\n",
    "    # create a dictionary and add special tokens\n",
    "    word2id = {}\n",
    "    word2id['<pad>'] = 0\n",
    "    word2id['<start>'] = 1\n",
    "    word2id['<end>'] = 2\n",
    "    word2id['<unk>'] = 3\n",
    "    \n",
    "    # if word frequency is less than 'min_word_count', then the word is discarded\n",
    "    words = [word for word, count in counter.items() if count >= min_word_count]\n",
    "    \n",
    "    # add the words to the word2id dictionary\n",
    "    for i, word in enumerate(words):\n",
    "        word2id[word] = i + 4\n",
    "    \n",
    "    return word2id\n",
    "\n",
    "\n",
    "def b_vocab(source_path, target_pad, min_word_count, src_out_path, trg_out_path):\n",
    "    \n",
    "    # build word2id dictionaries for source and target sequences\n",
    "    src_word2id = build_word2id(source_path, min_word_count)\n",
    "    trg_word2id = build_word2id(target_pad, min_word_count)\n",
    "    \n",
    "    # save word2id dictionaries\n",
    "    with open(src_out_path, 'w') as f:\n",
    "        json.dump(src_word2id, f)\n",
    "    with open(trg_out_path, 'w') as f:\n",
    "        json.dump(trg_word2id, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src.vocab\", \"r\") as f:\n",
    "    src_v = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trg.vocab\", \"r\") as f:\n",
    "    trg_v = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_vocab(\"en-de/train.tags.en-de.de\", \"en-de/train.tags.en-de.en\", 4, \"src.vocab\", \"trg.vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = get_loader(\"en-de/train.tags.en-de.de\", \"en-de/train.tags.en-de.en\", src_v, trg_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IWSLT German->English translation\n",
    "\n",
    "This notebook shows a simple example of how to use the transformer provided by this repo for one-direction translation. \n",
    "\n",
    "We will use the IWSLT 2016 De-En dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import sys\n",
    "from transformers import BaseTransformer\n",
    "\n",
    "from utils import Batch, BasicIterator\n",
    "from opt import NoamOpt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The below does some basic data preprocessing and filtering, in addition to setting special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single step over entire dataset, with tons of gradient accumulation to get batch sizes big enough for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del transformer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(BaseTransformer):\n",
    "    def __init__(\n",
    "        self, *args,\n",
    "    ):\n",
    "        super(TranslationModel, self).__init__(*args)\n",
    "\n",
    "    def forward_and_return_loss(self, criterion, sources, targets):\n",
    "        \"\"\"\n",
    "        Pass input through transformer and return loss, handles masking automagically\n",
    "        Args:\n",
    "            criterion: torch.nn.functional loss function of choice\n",
    "            sources: source sequences, [seq_len, bs]\n",
    "            targets: full target sequence, [seq_len, bs, embedding_dim]\n",
    "\n",
    "        Returns:\n",
    "            loss, transformer output\n",
    "        \"\"\"\n",
    "\n",
    "        batch = Batch(sources, targets, self.pad_idx)\n",
    "        seq_len, batch_size = batch.trg.size()\n",
    "        out = self.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = criterion(\n",
    "            out.contiguous().view(-1, out.size(-1)),\n",
    "            batch.trg_y.contiguous().view(-1),\n",
    "            ignore_index=self.pad_idx,\n",
    "        )\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "    def generate(self, source, source_mask, max_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: input sequence indices, [seq_len, bs,\n",
    "            source_mask: the source mask to prevent attending to <pad> tokens\n",
    "            max_len: maximum length\n",
    "\n",
    "        Returns:\n",
    "            generated translations\n",
    "        \"\"\"\n",
    "        memory = self.encoder(source, source_mask)\n",
    "        ys = torch.ones(1, source.size(1)).long().fill_(self.sos_idx).to(device)\n",
    "        # max target length is 1.5x * source + 10 to save compute power\n",
    "        for _ in range(int(1.5 * source.size(0)) - 1 + 10):\n",
    "            out = self.decoder(ys, memory, source_mask, Batch(ys, ys, 1).raw_mask)\n",
    "            out = self.fc1(out[-1].unsqueeze(0))\n",
    "            prob = F.log_softmax(out, dim=-1)\n",
    "            next_word = torch.argmax(prob, dim=-1)\n",
    "            ys = torch.cat([ys, next_word.detach()], dim=0)\n",
    "\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationTransformer(pl.LightningModule):\n",
    "    def __init__(self, transformer):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def forward(self, x, y, src_mask, trg_mask):\n",
    "        return self.transformer(x, y, src_mask, trg_mask)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        src, trg, src_mask, trg_mask, trg_y = batch\n",
    "        print(src.device, trg.device, src_mask.device, trg_mask.device, trg_y.device)\n",
    "        print(self.device)\n",
    "        #print(\"bs: \", src.size(0), src_mask.size(), trg_mask.size())\n",
    "        #print(src.device, self.device)\n",
    "        src_mask, trg_mask = src_mask.squeeze(), trg_mask.squeeze()\n",
    "\n",
    "            \n",
    "        out = transformer(src, \n",
    "                          trg, \n",
    "                          src_mask, \n",
    "                          trg_mask)\n",
    "        \n",
    "        loss = criterion(\n",
    "            out.contiguous().view(-1, out.size(-1)),\n",
    "            trg_y.transpose(0, 1).to(self.device).contiguous().view(-1)\n",
    "        )\n",
    "        \n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return result\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_vocab_size = len(src_v)\n",
    "output_vocab_size = len(trg_v)\n",
    "\n",
    "#input_vocab_size = 2000\n",
    "#output_vocab_size = 2000\n",
    "embedding_dim = 256\n",
    "n_layers = 4\n",
    "hidden_dim = 512\n",
    "n_heads = 8\n",
    "dropout_rate = .1\n",
    "transformer = TranslationModel(input_vocab_size, output_vocab_size, embedding_dim, \n",
    "                               n_layers,hidden_dim, n_heads, dropout_rate)\n",
    "\n",
    "# optimization is unstable without this step\n",
    "for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = TranslationTransformer(transformer)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 1)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, gpus=[0, 1], distributed_backend='dp')\n",
    "\n",
    "trainer.fit(m, dloader, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runs 10 epochs of the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_start = time.time()\n",
    "world_size = 0\n",
    "transformer = nn.DataParallel(transformer, device_ids=[0, 1, 2, 3])\n",
    "# torch.cuda.set_device(0)\n",
    "transformer = transformer.cuda()\n",
    "for i in range(10):\n",
    "    transformer.train()\n",
    "    t = time.time()\n",
    "    \n",
    "    loss = train_step(train_loader)\n",
    "    \n",
    "    print(\"Epoch {}. Loss: {}, \".format((i+1), str(loss)[:5], int(time.time() - t)))\n",
    "    print(\"Total time (s): {}, Last epoch time (s): {}\".format(int(time.time()- true_start), int(time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer, \"basic_translation.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, generations. \n",
    "\n",
    "\n",
    "The model by default uses greedy decoding for generation, and does not have incremental decoding. Currently, this leads to the transformer generating at about 1/2 the speed of Fairseq for short sequences. \n",
    "\n",
    "Implementing incremental decoding, however, makes the code for the attention function much harder to read, and has been left out for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "new_batch = next(iter(val_loader))\n",
    "inp = new_batch.src\n",
    "tra = new_batch.trg\n",
    "\n",
    "out = transformer.generate(inp, Batch(inp, inp, 1).src_mask, 120)\n",
    "for i in range(len(inp)):\n",
    "    print(\"Input sentence: \", end=\"\")\n",
    "    for j in range(1, inp.size(0)):\n",
    "        char = de.vocab.itos[inp[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")\n",
    "    print(\"\\nPredicted translation: \", end=\"\")\n",
    "    for j in range(1, out.size(0)):\n",
    "        char = en.vocab.itos[out[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")\n",
    "    print(\"\\nGround truth translation: \", end=\"\")\n",
    "    for j in range(1, tra.size(0)):\n",
    "        char = en.vocab.itos[tra[j, i]]\n",
    "        if char == \"</s>\": \n",
    "            break\n",
    "        print(char, end =\" \")    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
